负采样：
    功能：针对超大类别分类模型的一种简化/优化训练方式，效果和普通分类模型的训练是一样的。
    假定条件：
        -a. 假定类别数目是m个
        -b. 批次大小为N
        -c. 假设当前样本属于类别i
    网络结构：
        -a. 前面都是提取特征属性向量Z的网络结构
        -b. 最后一层就是简单的线性转换：S=torch.matmul(Z, W), S的形状是[N,m]表示每个样本属于m个类别的置信度
    分类模型的训练的方式：
        基于网络最后一层的输出构建交叉熵损失
            概率的计算：
                p1 = e^S1 / (e^S1 + e^S2 + .... + e^Sm)
                pi = e^Si / (e^S1 + e^S2 + .... + e^Sm)
            损失的计算:
                loss = -ln(pi)
            理解一下反向传播的过程：
                -a. 希望loss越小越好
                    --> pi越大越好
                    -->
                        分子部分: e^Si越大越好
                        分母部分: e^S1 + e^S2 + .... + e^Sm越小越好
                    -->
                        Si越大越好
                        S1、S2、...S(i-1)、S(i+1)、...Sm越小越好
                -b. 训练一定是一个不断重复迭代的过程，不断遍历的一个过程。
                    训练方式假设：
                        方式一(本来的分类的优化方式)：每次遇到当前样本的时候，都更新所有的S，满足上述条件
                        方式二(负采样)：
                            -1. 在第一次遇到当前样本的时候，将Si的值增大，同时将S1、S2、S3、S4减小，其它的Sj的值不考虑；
                            -2. 在第二次遇到当前样本的时候，将Si的值增大，同时将S4、S5、S7、S10减小，其它的Sj的值不考虑；
                            不断重复-1和-2，只是将减小的S随着批次进行更改.....
                            如果训练的批次足够多的时候，Si是不是就比所有其它的Sj的值都大了。 <==等价==>方式一
    普通分类模型的缺点：
        由于普通分类的计算方式是一个线性回归 + softmax概率转换的方式来计算概率，然后基于概率计算损失，最后返回传播
        时间复杂度：
            S=torch.matmul(Z, W) --> Z: [N,128], W[128,m]
                乘法的计算量就是: N*128*m
            pi = e^Si / (e^S1 + e^S2 + .... + e^Sm)
                计算量：m
            当类别数目m特别特别大的时候，比如：1亿，线性转换部分的计算量太大了
        缺点：当类别数目特别特别大的时候，正常的概率计算的计算量特别大，模型是无法训练的
    负采样：
        思路：
            -a. 没有必要计算所有负例的置信度值；
            -b. 正例的置信度值是每次都需要计算的；
            -c. 每个批次的训练都是让置信度在局部区域/局部类别范围内越大越好，并且通过随机选择负例的类别范围，通过迭代的思想最终让正例在全局所有类别中也保存最大值
        执行的步骤(迭代重复的)：
            -1. 从W中抽取正例对应的参数W0;
            -2. 从所有负例中随机抽取k个类别，然后提取对应的参数W1、W2、...、Wk；
            -3. 将所有参数拼接到一起形成一个新的参数U，U的形状就是[128, k+?]
            -4. 将原始的标签y映射到新的U参数对应的类别上；
            -5. 基于新的参数U做线性转换 + Softmax + 交叉熵损失，进行参数更新。
            NOTE:
                计算量只有: N*128*(k+?)
===============================================================================================
BPR
    例：
        -1. 系统给当前用户曝光的商品id列表为: [p3, p7, p12, p768, p1]
        -2. 用户在曝光列表中点击了商品: p7和p1
        -3. 数据转换
            p1, p768,   1
            p1, p3,     1
            p1, p12,    1
            p768, p1,   -1
            p768, p7,   -1
            p3, p1,     -1
            p3, p7,     -1
            p7, p768,   1
            p7, p3,     1
            p7, p12,    1
            p12, p1,    -1
            p12, p7,    -1
            NOTE: 当标签为1，表示第一个物品应该排在第二个物品的前面；标签为-1，表示第一个物品应该排在第二个物品的后面。
===============================================================================================